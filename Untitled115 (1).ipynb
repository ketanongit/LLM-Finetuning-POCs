{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8QrJmVyTe0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b52b9cd-c162-4fe7-82c5-3b254557ac63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Complete Fine-tuning and GGUF Conversion Pipeline for Google Colab\n",
        "# This notebook fine-tunes a small instruct model (Qwen2.5-0.5B) and converts to GGUF\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: INSTALLATION AND SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q transformers datasets accelerate peft trl bitsandbytes\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Disable wandb completely\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Clone and install llama.cpp for GGUF conversion\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "!cd llama.cpp && make\n",
        "\n",
        "# Install additional requirements for conversion\n",
        "!pip install -q gguf numpy sentencepiece protobuf\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import os\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: SAMPLE DATASET CREATION\n",
        "# ============================================================================\n",
        "\n",
        "# Create sample training data - modify this with your actual data\n",
        "sample_data = [\n",
        " {\n",
        "        \"instruction\": \"What is my name?\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"your name is ketan\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"How do i cook eggs\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"in the electric your mom gave you\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# Save sample data to JSON file\n",
        "with open('training_data.json', 'w') as f:\n",
        "    json.dump(sample_data, f, indent=2)\n",
        "\n",
        "print(\"Sample dataset created with\", len(sample_data), \"examples\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: DATA PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "def format_instruction(sample):\n",
        "    \"\"\"Format the sample into a training prompt\"\"\"\n",
        "    if sample[\"input\"]:\n",
        "        return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Input:\\n{sample['input']}\\n\\n### Response:\\n{sample['output']}\"\n",
        "    else:\n",
        "        return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']}\"\n",
        "\n",
        "# Load and format dataset\n",
        "def load_and_format_data(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Format each sample\n",
        "    formatted_data = []\n",
        "    for sample in data:\n",
        "        formatted_text = format_instruction(sample)\n",
        "        formatted_data.append({\"text\": formatted_text})\n",
        "\n",
        "    return Dataset.from_list(formatted_data)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_and_format_data('training_data.json')\n",
        "print(\"Dataset loaded:\", dataset)\n",
        "print(\"\\nSample formatted text:\")\n",
        "print(dataset[0]['text'])\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: MODEL AND TOKENIZER SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Choose a small model - Qwen2.5-0.5B is good for Colab\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load model with 4-bit quantization to save memory\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: TOKENIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize the text\n",
        "    tokens = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_overflowing_tokens=False,\n",
        "    )\n",
        "\n",
        "    # Set labels for language modeling (copy of input_ids)\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n",
        "# Tokenize dataset\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Dataset tokenized\")\n",
        "print(\"Sample tokenized length:\", len(tokenized_dataset[0][\"input_ids\"]))\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: LORA CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Configure LoRA for efficient fine-tuning\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                   # Rank\n",
        "    lora_alpha=32,          # Alpha parameter\n",
        "    target_modules=[        # Target modules for LoRA\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: TRAINING CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen-finetuned\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=10,\n",
        "    max_steps=50,  # Keep small for demo\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=25,\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,\n",
        "    report_to=[],  # Disable all reporting including wandb\n",
        "    disable_tqdm=False,\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    pad_to_multiple_of=8,  # Pad to multiple of 8 for efficiency\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(\"./qwen-finetuned\")\n",
        "\n",
        "print(\"Training completed and model saved!\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: TEST THE FINE-TUNED MODEL\n",
        "# ============================================================================\n",
        "\n",
        "# Load the fine-tuned model for testing\n",
        "# We need to reload the base model and apply PEFT\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model\n",
        "base_model_for_testing = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load PEFT model\n",
        "test_model = PeftModel.from_pretrained(base_model_for_testing, \"./qwen-finetuned\")\n",
        "\n",
        "def generate_response(instruction, input_text=\"\"):\n",
        "    if input_text:\n",
        "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
        "    else:\n",
        "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(test_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = test_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"### Response:\\n\")[-1]\n",
        "\n",
        "# Test the model\n",
        "test_instruction = \"What is artificial intelligence?\"\n",
        "response = generate_response(test_instruction)\n",
        "print(f\"Question: {test_instruction}\")\n",
        "print(f\"Response: {response}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: CONVERT TO GGUF FORMAT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Converting model to GGUF format...\")\n",
        "\n",
        "# First, merge LoRA weights into base model\n",
        "from peft import PeftModel\n",
        "\n",
        "# Define the original model name again (in case it got overwritten)\n",
        "original_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "# Load base model without quantization for merging\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    original_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load LoRA model\n",
        "peft_model = PeftModel.from_pretrained(base_model, \"./qwen-finetuned\")\n",
        "\n",
        "# Merge and save\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"./qwen-merged\", safe_serialization=True)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer_for_conversion = AutoTokenizer.from_pretrained(original_model_name)\n",
        "tokenizer_for_conversion.save_pretrained(\"./qwen-merged\")\n",
        "\n",
        "print(\"Model merged and saved to ./qwen-merged\")\n",
        "\n",
        "# Convert to GGUF using llama.cpp\n",
        "print(\"Converting to GGUF...\")\n",
        "\n",
        "# Convert to GGUF format\n",
        "!python llama.cpp/convert_hf_to_gguf.py ./qwen-merged --outfile qwen-finetuned.gguf --outtype f16\n",
        "\n",
        "print(\"Conversion to GGUF completed!\")\n",
        "print(\"GGUF file: qwen-finetuned.gguf\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 12: DOWNLOAD FILES (FOR COLAB)\n",
        "# ============================================================================\n",
        "\n",
        "# Download the GGUF files to local machine\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading GGUF files...\")\n",
        "files.download('qwen-finetuned.gguf')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WoRx0K4tT7B-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}